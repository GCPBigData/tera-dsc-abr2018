{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio:  California Housing Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos:\n",
    "\n",
    "O objetivo desse desafio é analisar os dados do case e estruturar uma Feature Engineering básica apenas com os dados existentes, sem transformar ou combinar features ou mesmo adicionar informações externas. \n",
    "\n",
    "Ao final do desafio, será treinado um modelo de regressão linear com as features obtidas. Esse modelo será testado contra uma massa de teste, separada previamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre o Case\n",
    "\n",
    "### Case baseado no dataset do Kaggle: \"California Housing Prices\"\n",
    "\n",
    "Esse desafio é baseado em um dataset aberto do Kaggle ([https://www.kaggle.com](https://www.kaggle.com)) de 2018, de onde é possível estimar o preço de um imóvel pertencente a uma dada região na Califórnia. \n",
    "\n",
    "O dataset original foi extraído do repositório StatLib, que não está mais disponível. Os dados que o compôem foram retirados do Censo realizado na Califórnia em 1990 e modificado para servir como base de treinamento.\n",
    "\n",
    "\n",
    "Link para o dataset no Kaggle: [https://www.kaggle.com/harrywang/housing/data](https://www.kaggle.com/harrywang/housing/data)\n",
    "\n",
    "\n",
    "### Descrição dos Dados Originais:\n",
    "\n",
    "#### Tamanho do Dataset:\n",
    "\n",
    "* `20.640` data points\n",
    "\n",
    "#### Variável dependente:\n",
    "\n",
    "* `median_house_value`:  (float) variável dependente com o valor da mediana do preço de imóvel na região\n",
    "\n",
    "#### Features: \n",
    "\n",
    "* `longitude`/`latitude`: (floats) posição global da região\n",
    "* `housing_median_age`: (float) mediana da idade (em anos) das casas da região\n",
    "* `total_rooms`: (float) total de aposentos da região\n",
    "* `total_bedrooms`: (float) total de quartos da região\n",
    "* `population`: (float) população total da região\n",
    "* `households`: (float) quantidade total de imóveis da região\n",
    "* `median_income`: (float) mediana do salário (por hora) de uma pessoa na região\n",
    "* `ocean_proximity`: (string) categorias relativas à distância do oceano\n",
    "\n",
    "É interessante ressaltar que `207` valores foram retirados aleatoriamnete da feature `total_bedrooms` por motivos acadêmicos.\n",
    "\n",
    "\n",
    "### Modificação dos dados para o Desafio:\n",
    "\n",
    "Para tornar o desafio mais fácil de avaliar, a massa de dados original foi dividida em duas massas, uma para treino e outra para teste, ambas contendo `10.320` elementos.\n",
    "\n",
    "Também foram removidos aleatoriamente ainda mais valores da massa de dados e em mais features. Isso torna o processo de Feature Engineering mais instrutivo, pois aproxima-se da realidade do trabalho de um Data Scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer, PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/california_housing_train.csv\", index_col=0)\n",
    "x_train = dataset.drop([\"median_house_value\"], axis=1)\n",
    "y_train = dataset[[\"median_house_value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape: {x_train.shape}\")\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape: {y_train.shape}\")\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/california_housing_test.csv\", index_col=0)\n",
    "x_test = dataset.drop([\"median_house_value\"], axis=1)\n",
    "y_test = dataset[[\"median_house_value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape: {x_test.shape}\")\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"shape: {y_test.shape}\")\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Features Numéricas\n",
    "\n",
    "Features numéricas são as primeiras a serem tratadas, por serem as mais fáceis de compreender e de relacionar com o problema. As seções a seguir focam no tratamento e limpeza dessas features, o primeiro passo na engenharia de features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribuição das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "numerical_cols = [?, ?, ..., ?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descrevendo as distribuições\n",
    "cuts = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "x_train[numerical_cols].describe(percentiles=cuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tomada de decisão, é sempre mais informativo usar visualizações. Com elas é possível observar o formato da distribuição (normal, exponencial, logarítmica, etc) e os outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando as distribuições com histogramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[numerical_cols].hist(bins=50, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Detecção e Tratamento de Nulos\n",
    "\n",
    "Conhecendo a distribuição das features, já é possível traçar estratégias de tratamento de valores nulos. A primeira tarefa é detectar a proporção de nulos nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Verificar quantidade de Nulos em cada feature\n",
    "* Entender que Nulos podem aparecer também nos dados de teste, portanto apenas descartar o dado não resolve\n",
    "* Entender que medida (média ou mediana) deve ser imputada em cada feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_null = x_train[numerical_cols].isnull()\n",
    "null_data = pd.DataFrame({\n",
    "    \"count\": x_null.sum(),\n",
    "    \"mean\": x_null.mean()\n",
    "})\n",
    "null_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente a ocorrência de valores nulos ocorre por algum ruído no processo de obtenção de dados, o que significa que muito provavelmente eles ocorrerão também em dados de produção.\n",
    "\n",
    "Existem algumas técnicas de tratamento de valores nulos:\n",
    "\n",
    "- Atribuir um valor pafrão fora da distribuição;\n",
    "- Criar modelos para inferir os valores a partir das outras features;\n",
    "- Imputar um valor referente à distribuição:\n",
    "    - média\n",
    "    - mediana\n",
    "    \n",
    "Para features numéricas em que conhecemos a distribuição mas não foram tratados os outliers, o mais recomendado é utilizar a **mediana**, cujo valor é pouco afetado por outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumericalFeaturesImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Classe de Feature Transformer baseada em um Imputer de Mediana.\n",
    "        Esse imputer mantém a entrada X como um DataFrame em vez de transformar em numpy.array.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.imputer = Imputer(strategy=\"median\")\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.imputer.fit(X.loc[:, self.columns])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_t = X.copy()\n",
    "        X_t.loc[:, self.columns] = self.imputer.transform(X_t.loc[:, self.columns])\n",
    "        return X_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check 1: tranformation keeps columns \n",
    "numerical_imputer = NumericalFeaturesImputer(numerical_cols)\n",
    "x_train = numerical_imputer.fit_transform(x_train)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check 2: NUll data replaced\n",
    "x_null = x_train[numerical_cols].isnull()\n",
    "null_data = pd.DataFrame({\n",
    "    \"count\": x_null.sum(),\n",
    "    \"mean\": x_null.mean()\n",
    "})\n",
    "null_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação Logarítmica de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os histogramas, já fica evidente que a distribuição de algumas features é exponencial. A análise de outliers desse tipo de distribuição pode ser prejudicada pela alta concentração de elementos em uma pequena parte do domínio. \n",
    "\n",
    "Uma forma de corrigir essa distorção é transformar esses dados com a função **logarítmica**; a transformação com essa função torna a distribuição das features mais próxima da normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "log_cols = [?, ?, ..., ?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogFeaturesTransform(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Classe de Feature Transformer para aplicar log em valores numéricos.\n",
    "        Esse imputer mantém a entrada X como um DataFrame em vez de transformar em numpy.array.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_t = X[self.columns].apply(np.log).rename(columns=lambda c: f\"log_of_{c}\")\n",
    "        return X.join(X_t)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda é cedo para descartar as features originais tratadas, pois elas podem ter ainda algum poder preditivo que pode ficar ocluso pela transformação. O descarte de features normalmente é feito quando se detecta multi-colinearidade entre as features ou durante uma etapa de _feature selection_.\n",
    "\n",
    "Por hora, as novas features serão integradas ao dataset original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = LogFeaturesTransform(log_cols).fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_cols = [f\"log_of_{c}\" for c in log_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[log_cols].hist(bins=50, figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Detecção e Remoção de Outliers\n",
    "\n",
    "Outliers podem deformar a percepção do domínio para o aprendizado de um modelo linear, impedindo o mesmo de encontrar uma solução correta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificando os Box Plots para observar os Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Escreva a solução aqui \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicando os cortes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inicialização\n",
    "keep_index = pd.Series(index=x_train.index, data=True)\n",
    "cuts_table = pd.DataFrame(columns=[\"count\", \"percent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Template para os cortes: replique essa célula para cada corte definido no item anterior\n",
    "\n",
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "\n",
    "feat = \"?\"\n",
    "lim_inf = ?\n",
    "lim_sup = ?\n",
    "\n",
    "cuts_index = (x_train[feat] < lim_inf) | ((x_train[feat] > lim_sup)) \n",
    "cuts_table = cuts_table.append(\n",
    "    pd.DataFrame(\n",
    "        index=[f\"{lim_inf} <= {feat} <= {lim_sup}\"],\n",
    "        columns=[\"count\", \"percent\"],\n",
    "        data=[[cuts_index.sum(), cuts_index.mean()]]\n",
    "    )\n",
    ")\n",
    "keep_index &= ~cuts_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizando os cortes\n",
    "cuts_table.append(\n",
    "    pd.DataFrame(\n",
    "        index=[f\"Total Elements Cut\"],\n",
    "        columns=[\"count\", \"percent\"],\n",
    "        data=[[(~keep_index).sum(), (~keep_index).mean()]]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicação do corte total\n",
    "ori_size = x_train.shape[0]\n",
    "\n",
    "x_train = x_train.loc[keep_index]\n",
    "\n",
    "new_size = x_train.shape[0]\n",
    "\n",
    "print(f\"Size of 'x_train' before Cuts:\\t {ori_size}\")\n",
    "print(f\"Size of 'x_train' after Cuts:\\t {new_size} (-{100. * (~keep_index).mean(): 0.2f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Feature Categórica\n",
    "\n",
    "Features Categóricas são um pouco mais interessantes de tratar do que as numéricas, já que existem muitas maneiras de se transformar textos ou símbolos em valores numéricos. \n",
    "\n",
    "Vale lembrar que todo modelo de machine learning compreende o mundo através de valores numéricos, por serem modelos matemáticos de busca de solução ótima. Alguns frameworks atuais permitam que se coloquem valores simbólicos ou de texto marcados com a tag 'category' diretamente no dataset, mas por trás o proprio framework transforma esses dados em números."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise da Distribuição das Categorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação da quantidade de dados em cada categoria\n",
    "\n",
    "É interessante verificar a quantidade de dados em cada categoria, pois categoriass mal representadas podem criar conceitos enviesados do modelo sobre o domínio. Por exemplo, em um dataset em que uma categoria só ocorra uma única vez e a variável dependente exatamente nesse elemento seja muito alta, um modelo treinado pode assumir que a presença dessa categoria já indique uma saída alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando a distribuiçlão das categorias na massa de treino\n",
    "x_train[\"ocean_proximity\"].fillna(\" - NaN - \").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nem sempre é possível verificar a distribuição exata dos dados de produção, mas a massa de teste normalmente dá uma boa aproximação dela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando a distribuiçlão das categorias na massa de teste\n",
    "x_test[\"ocean_proximity\"].fillna(\" - NaN - \").value_counts().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definir o tratamento de categorias pouco representativos:\n",
    "\n",
    "Pode-se observar que a categoria `ISLAND` tem uma representatividade mínima em todo o dataset, tornando essa categoria a única candidata à eliminação. Como já existem elementos com a categoria nula nesse dataset, a melhor estratégia é juntar a categoria `ISLAND` aos nulos e tratá-los (próxima seção)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Detecção e Tratamento de Nulos\n",
    "\n",
    "Como já foram identificados elementos em que a categoria é nula, é importante tratar esses elementos apropriadamente.\n",
    "\n",
    "Existem algumas estratégias para tratamento de nulos:\n",
    "\n",
    "- Criar uma categoria `NULL` e usar como um símbolo válido do sistema;\n",
    "- Criar modelos para inferir os valores a partir das outras features;\n",
    "- Imputar um valor referente à distribuição: a `moda` (valor com a maior frequência)\n",
    "\n",
    "Como existe a informação de que a variável categórica foi criada a partir da anotação manual do autor do dataset e que o mesmo utilizou as coordenadas `latitude` e `longitude`, a melhor estratégia é criar um `Imputer` que **busque a categoria do elemento usando as coordenadas geográfica**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class CategoricalFeaturesImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Classe de Feature Transformer baseada em um Imputer Categórico que busca a categoria \n",
    "        do elemento mais próximo.\n",
    "        Esse imputer mantém a entrada X como um DataFrame em vez de transformar em numpy.array.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, valid_categories):\n",
    "        self.valid_categories = valid_categories\n",
    "        self.imputer = ?\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        target_feat = ?\n",
    "        inputs_cols = [?, ?]\n",
    "        \n",
    "        index = X[target_feat].isin(self.valid_categories)\n",
    "        \n",
    "        x_train = X.loc[index, inputs_cols]\n",
    "        y_train = X.loc[index, target_feat]\n",
    "        \n",
    "        self.imputer.fit(x_train, y_train)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        target_feat = ?\n",
    "        inputs_cols = [?, ?]\n",
    "        \n",
    "        X_t = X.copy()\n",
    "        X_t.loc[:, target_feat] = self.imputer.predict(X_t.loc[:, inputs_cols])\n",
    "        return X_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "\n",
    "valid_categories = [?, ?, ..., ?]\n",
    "\n",
    "x_valid = x_test[x_test.ocean_proximity.isnull()]\n",
    "\n",
    "obj = CategoricalFeaturesImputer(valid_categories).fit(x_train)\n",
    "obj.transform(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Transformação em Dummy Features\n",
    "\n",
    "A transformação em Dummy Features é a técnica em que os labels são formatados em dados categóricos consumíveis pelo modelo. O formato mais comum é usar uma representação em que cada label é uma nova feature binária onde o valor é **um** onde a feature é igual ao label e **zero** em todo o resto. \n",
    "\n",
    "Por exemplo, a transformação do vetor `[a, b, d, b, e, a, c]` seria da forma:\n",
    "\n",
    "| a | b | c | d | e |\n",
    "|---|---|---|---|---|\n",
    "| 1 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 0 | 0 |\n",
    "\n",
    "Em modelos lineares existe uma regra de ouro que uma das classes deve permanecer como 'Base' para não haver uma feature linearmente dependente dentro do dataset. Alguns modelos tratam esse problema internamente, mas ainda assim é uma boa prática a ser seguida.\n",
    "\n",
    "Como é uma etapa de pré-processamento, essa transformação também deve ser feita como uma Feature Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CategoricalToDummyFeaturesTransform(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Classe de Feature Transformer que transforma labels na representação Dummy.\n",
    "        Esse imputer mantém a entrada X como um DataFrame em vez de transformar em numpy.array.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, categories):\n",
    "        self.categories = [f\"ocean_proximity: {c}\" for c in categories]\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):        \n",
    "        X_t = X.drop(\"ocean_proximity\", axis=1)\n",
    "        \n",
    "        dummy = pd.get_dummies(\n",
    "            X[\"ocean_proximity\"], \n",
    "            prefix=\"ocean_proximity\",\n",
    "            prefix_sep=\": \"\n",
    "        ) \n",
    "        return X_t.join(dummy.loc[:, self.categories])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "\n",
    "categories = [?, ?, ..., ?]\n",
    "\n",
    "CategoricalToDummyFeaturesTransform(categories).transform(x_train).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Combinação Manual de Features\n",
    "\n",
    "É uma prática comum agrupar manualmente features por assunto ou grupos de conhecimento, principalmente quando se tem um bom conhecimento do domínio de aplicação. \n",
    "\n",
    "Algumas features numéricas nesse dataset são bem relacionadas entre si, sendo imediato pensar em combiná-las. Para manter o padrão no pré-processamento, isso será feito em uma classe de Feature Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "\n",
    "class ManuallyCraftedFeaturesTransform(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Classe de Feature Transformer que aplica transformações manuais.\n",
    "        Esse imputer mantém a entrada X como um DataFrame em vez de transformar em numpy.array.\n",
    "    \"\"\"    \n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):        \n",
    "        X_t = X.copy()\n",
    "        \n",
    "        # Início das transformações manuais; \n",
    "        # repita as linhas abaixo para cada transformação\n",
    "        \n",
    "        X_t.loc[:, \"?\"] = ? * ? / ? + ? \n",
    "        \n",
    "        # Fim das transformações\n",
    "        \n",
    "        return X_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "ManuallyCraftedFeaturesTransform().transform(x_train).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Combinação Polinomial de Features\n",
    "\n",
    "Uma outra técnica muito utilizada em feature engineering é criar novas features a partir da combinação polinomial de features antigas. Dessa forma, uma solução linear pode ser extendida para uma solução não linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolynomialFeaturesTransform(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Classe de Feature Transformer que cria features não lineares usando Transformação Polinomial.\n",
    "        Esse imputer mantém a entrada X como um DataFrame em vez de transformar em numpy.array.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features, degree):\n",
    "        self.features = features\n",
    "        self.polifeat = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.polifeat.fit(X[self.features])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):        \n",
    "        X_t = X.drop(self.features, axis=1)\n",
    "        \n",
    "        x_poli = pd.DataFrame(\n",
    "            index=X.index,\n",
    "            columns=self.polifeat.get_feature_names(self.features),\n",
    "            data=self.polifeat.transform(X[self.features])\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return X_t.join(x_poli)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "\n",
    "features = [?, ?, ..., ?]\n",
    "\n",
    "PolynomialFeaturesTransform(features, 4).fit_transform(x_train).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E) Dados Externos: Pontos de Interesse\n",
    "\n",
    "Uma das técnicas que mais trazem informação para o modelo é a inclusão de dados externos. \n",
    "\n",
    "Para essa seção, deve-se escolher alguns pontos de interesse da Califórina e arredores para calcular a distância euclidiana da Latitude/Longitude do data point a esses pontos de interesse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "\n",
    "class PointsOfInterestFeaturesTransform(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Classe de Feature Transformer que cria features baseadas na distância a pontos de interesse.\n",
    "        Esse imputer mantém a entrada X como um DataFrame em vez de transformar em numpy.array.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.poi = {\n",
    "            \"?\": (?, ?),\n",
    "            \"?\": (?, ?),\n",
    "            \"?\": (?, ?)\n",
    "        }\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):        \n",
    "        X_t = X.copy()        \n",
    "        x_poi = pd.DataFrame(index=X_t.index)\n",
    "        for k in self.poi:\n",
    "            distance_lat = (X_t.latitude - self.poi[k][0]) ** 2\n",
    "            distance_lon = (X_t.longitude - self.poi[k][1]) ** 2\n",
    "            x_poi.loc[:, f\"Distance to {k}\"] = (distance_lat + distance_lon) ** 0.5        \n",
    "        return X_t.join(x_poi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PointsOfInterestFeaturesTransform().fit_transform(x_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F) Treinamento e Avaliação de um Modelo Linear\n",
    "\n",
    "O modelo de machine learning é apenas a parte final de um pipeline de processamentos; o pipeline completo é formado por todas as etapas de pré-processamento desde o dado bruto até as etapas de normalização e redução de dimensionalidade, finalizado pelo modelo preditivo.\n",
    "\n",
    "O framework `Scikit-Learn` implementa uma ferramenta que permite a montagem de um pipeline completo, que pode ser treinado e usado para predição como um objeto único, que pode ser inclusive salvo em um arquivo. Isso permite que todo o pipeline possa ser exportado para produção sem ser reimplementado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reload das massas de Treino e de Teste\n",
    "\n",
    "As massas de dados de Treino e de Teste serão carregadas novamente para que seja aplicado o pipeline de pré-processamento em ambos desde o princípio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/california_housing_train.csv\", index_col=0)\n",
    "x_train = dataset.drop([\"median_house_value\"], axis=1)\n",
    "y_train = dataset[[\"median_house_value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/california_housing_test.csv\", index_col=0)\n",
    "x_test = dataset.drop([\"median_house_value\"], axis=1)\n",
    "y_test = dataset[[\"median_house_value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Limpeza do dataset de treino\n",
    "\n",
    "Devem ser aplicados os mesmos cortes definidos no item A, para que apenas os dados dentro da distribuição correta sejam usados para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.loc[keep_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.loc[keep_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pipeline de Pré-Processamento\n",
    "\n",
    "A construção do pipeline deve incluir:\n",
    "\n",
    "* Todas as etapas de pré-processamento\n",
    "* Normalização (Z-Score)\n",
    "* Modelo Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Complete os espaços com ? \"\"\"\n",
    "numerical_features = [?, ?, ..., ?]\n",
    "log_transform_features = [?, ?, ..., ?]\n",
    "imputer_categories = [?, ?, ..., ?]\n",
    "dummy_categories = [[?, ?, ..., ?]\n",
    "poly_features = [?, ?, ..., ?]\n",
    "poly_degree = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"numerical_imputer\",          NumericalFeaturesImputer(numerical_features)),\n",
    "    (\"logarithmic_transform\",      LogFeaturesTransform(log_transform_features)),\n",
    "    (\"categorical_imputer\",        CategoricalFeaturesImputer(imputer_categories)),\n",
    "    (\"dummy_category_transform\",   CategoricalToDummyFeaturesTransform(dummy_categories)),\n",
    "    (\"manually_crafted_transform\", ManuallyCraftedFeaturesTransform()),\n",
    "    (\"polynomial_feats_transform\", PolynomialFeaturesTransform(poly_features, poly_degree)),\n",
    "    (\"points_of_interest\",         PointsOfInterestFeaturesTransform()),\n",
    "    (\"zscore\",                     StandardScaler()),\n",
    "    (\"predictor\",                  ElasticNet()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Treinar e avaliar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainamento\n",
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# avaliação do modelo na massa de treino\n",
    "y_true = y_train\n",
    "y_pred = pipeline.predict(x_train)\n",
    "mse_tr = mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "r2_tr = r2_score(y_true=y_true, y_pred=y_pred)\n",
    "r2_tr = 1 - (1-r2_tr)*(len(y_train)-1)/(len(y_train)-x_train.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# avaliação do modelo na massa de teste\n",
    "y_true = y_test\n",
    "y_pred = pipeline.predict(x_test)\n",
    "mse_te = mean_squared_error(y_true=y_true, y_pred=y_pred)\n",
    "r2_te = r2_score(y_true=y_true, y_pred=y_pred)\n",
    "r2_te = 1 - (1-r2_te)*(len(y_test)-1)/(len(y_test)-x_test.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tabela final mostrando os resultados\n",
    "pd.DataFrame(\n",
    "    index=[\"train\", \"test\"],\n",
    "    columns=[\"MSE\", \"R^2\"],\n",
    "    data=[\n",
    "        [mse_tr, r2_tr],\n",
    "        [mse_te, r2_te]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabela final mostrando os resultados\n",
    "pd.DataFrame(\n",
    "    index=[\"train\", \"test\"],\n",
    "    columns=[\"MSE\", \"R^2\"],\n",
    "    data=[\n",
    "        [mse_tr, r2_tr],\n",
    "        [mse_te, r2_te]\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
